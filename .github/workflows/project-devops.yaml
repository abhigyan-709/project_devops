name: Docker Build, Push to AWS ECR & Terraform Setup

on:
  push:
    branches:
      - main  # Trigger on push to the main branch

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest

    steps:
      # Checkout the code from the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up AWS credentials to access ECR and Terraform
      - name: Set up AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      # Install Terraform
      - name: Install Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      # Initialize Terraform
      - name: Terraform Init
        run: terraform init -backend-config="bucket=project-devops-state-bucket" -backend-config="key=terraform.tfstate" -backend-config="region=ap-south-1"
        working-directory: ./infrastructure

      # Plan Terraform changes
      - name: Terraform Plan
        run: terraform plan
        working-directory: ./infrastructure

      # Apply Terraform changes (only if the plan shows no errors)
      - name: Terraform Apply
        run: terraform apply -auto-approve
        working-directory: ./infrastructure
        env:
          TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      # Install Docker Compose
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
          docker-compose --version   

      # Build the Docker image    
      - name: Build the Docker image
        run: docker build --build-arg MONGO_URI=${{ secrets.MONGO_URI }} -t project-devops -f ./app/Dockerfile ./app

      # Log in to Amazon ECR
      - name: Log in to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
        with:
          mask-password: true  # Mask the password
          registry-type: private
          skip-logout: false
        env:
          AWS_DEFAULT_REGION: ap-south-1
          AWS_REGION: ap-south-1
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      # Tag the Docker image
      - name: Tag the Docker image
        run: docker tag project-devops:latest 774305585645.dkr.ecr.ap-south-1.amazonaws.com/project-devops:latest

      # Push Docker image to ECR
      - name: Push Docker image to ECR
        run: docker push 774305585645.dkr.ecr.ap-south-1.amazonaws.com/project-devops:latest

        
  # Deploy job
  deploy:
    runs-on: ubuntu-latest
    needs: build  # Ensure the deploy job runs after the build job

    steps:
      # Checkout the code from the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up SSH key for EC2
      - name: Set up SSH key for EC2
        run: |
          echo "${{ secrets.EC2_SSH_KEY }}" > ec2-key.pem
          chmod 600 ec2-key.pem
          eval $(ssh-agent -s)
          ssh-add ec2-key.pem

      # Retrieve kubeconfig from EC2
      - name: Retrieve kubeconfig from EC2 Master
        run: |
          # SSH into the EC2 instance and retrieve the kubeconfig
          ssh -o StrictHostKeyChecking=no -i ec2-key.pem ubuntu@ec2-13-201-79-164.ap-south-1.compute.amazonaws.com "sudo cat /etc/kubernetes/admin.conf" > kubeconfig
          
          # Update the kubeconfig to use the public IP instead of the private IP
          sed -i 's/172.31.37.113/ec2-13-201-79-164.ap-south-1.compute.amazonaws.com/' kubeconfig
          
          # Set the KUBECONFIG environment variable to use the retrieved kubeconfig
          export KUBECONFIG=kubeconfig
          
          # Check if kubectl is working
          kubectl get nodes --insecure-skip-tls-verify  # Use this flag as a workaround to skip SSL verification

      - name: Copy Kubernetes deployment files to EC2 Worker Node
        run: |
          ssh -o StrictHostKeyChecking=no -i ec2-key.pem ubuntu@ec2-13-201-83-111.ap-south-1.compute.amazonaws.com "mkdir -p /home/ubuntu/k8-deployments"
          scp -o StrictHostKeyChecking=no -i ec2-key.pem ./k8-deployments/deployment.yaml ubuntu@ec2-13-201-83-111.ap-south-1.compute.amazonaws.com:/home/ubuntu/k8-deployments/
          scp -o StrictHostKeyChecking=no -i ec2-key.pem ./k8-deployments/services.yaml ubuntu@ec2-13-201-83-111.ap-south-1.compute.amazonaws.com:/home/ubuntu/k8-deployments/



      - name: Install and Configure AWS CLI on Worker Node, Create ECR Secret
        run: |
          ssh -o StrictHostKeyChecking=no -i "ec2-key.pem" ubuntu@ec2-13-201-83-111.ap-south-1.compute.amazonaws.com << 'EOF'
            # Step 1: Update package repository and install AWS CLI
            sudo apt-get update
            sudo apt-get install -y awscli

            # Step 2: Configure AWS CLI with provided credentials (using GitHub secrets)
            aws configure set aws_access_key_id "${{ secrets.AWS_ACCESS_KEY_ID }}"
            aws configure set aws_secret_access_key "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
            aws configure set region "ap-south-1"

            # Step 3: Create the ECR secret using AWS CLI and kubectl
            "kubectl delete secret ecr-secret --ignore-not-found && \
             aws ecr get-login-password --region ap-south-1 | \
             kubectl create secret docker-registry ecr-secret \
             --docker-server=774305585645.dkr.ecr.ap-south-1.amazonaws.com \
             --docker-username=AWS \
             --docker-password=\$(cat) \
             --docker-email=your-email@example.com"

            # Step 4: Verify the secret creation
            kubectl get secret ecr-secret
          EOF

      

      # Deploy to Kubernetes on EC2
      - name: Deploy to Kubernetes on EC2 Worker Node
        run: |
          # SSH into the EC2 master node and deploy Kubernetes resources
          ssh -o StrictHostKeyChecking=no -i ec2-key.pem ubuntu@ec2-13-201-83-111.ap-south-1.compute.amazonaws.com << 'EOF'
            kubectl delete -f ./k8-deployments/deployment.yaml --ignore-not-found
            kubectl delete -f ./k8-deployments/services.yaml --ignore-not-found
            kubectl apply -f ./k8-deployments/deployment.yaml
            kubectl apply -f ./k8-deployments/services.yaml
            # kubectl rollout restart deployment k8-app-deployment
            # you can also use kubectl apply -f ./k8-deployments/ to apply all the resources in the directory
            # you can also use kubectl rollout restart deployment k8-app-deployment to restart the deployment
          EOF


# the above keys are securely stored in the github secrets & are used in the above workflows.
# The above workflows are triggered on the push to the main branch of the repository.
# The above workflows are used to build the docker image, push it to the AWS ECR, and deploy the application on the Kubernetes cluster running on the EC2 instances.
# The above workflows are used to automate the CI/CD process for the project.
# The above workflows are used to automate the deployment of the application on the Kubernetes cluster running on the EC2 instances.